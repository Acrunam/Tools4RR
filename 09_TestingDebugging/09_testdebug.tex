\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}

\input{../LaTeX/header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Testing and debugging}
\subtitle{Tools for Reproducible Research}
\author{\href{http://www.biostat.wisc.edu/~kbroman}{Karl Broman}}
\institute{Biostatistics \& Medical Informatics, UW{\textendash}Madison}
\date{\href{http://www.biostat.wisc.edu/~kbroman}{\tt \scriptsize \color{foreground} biostat.wisc.edu/{\textasciitilde}kbroman}
\\[-4pt]
\href{http://github.com/kbroman}{\tt \scriptsize \color{foreground} github.com/kbroman}
\\[-4pt]
\href{https://twitter.com/kwbroman}{\tt \scriptsize \color{foreground} @kwbroman}
\\[-4pt]
{\scriptsize Course web: \href{http://bit.ly/tools4rr}{\tt bit.ly/tools4rr}}
}

\begin{document}

{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

\note{We spend a lot of time debugging. We'd spend a lot less time if
  we tested our code properly.

  We want to get the right answers. We can't be sure that we've done
  so without testing our code.

  Set up a formal testing system, so that you can be
  confident in your code, and so that problems are identified and
  corrected early.

  Even with a careful testing system, you'll still spend time
  debugging. Debugging can be frustrating, but the right tools and
  skills can speed the process.
}
} }




\begin{frame}[c]{}

\centerline{"I tried it, and it worked."}

\note{This is about the limit of most programmers' testing efforts.

{\nhilit But}: Does it {\nvhilit still} work? Can you reproduce what you did?
With what variety of inputs did you try?
}

\end{frame}

\begin{frame}[c]{}

\vspace{100pt}

"It's not that we don't test our code, it's that we don't store our
tests so they can be re-run automatically."

\vspace{36pt}

\hfill \lolit {\textendash} Hadley Wickham

\vspace{60pt}
{\footnotesize
\hfill \href{http://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}{R Journal 3(1):5{\textendash}10, 2011}
}

\note{
}
\end{frame}

\begin{frame}{Types of tests}

\bbi
\onslide<2->{\item Check inputs
  \bi
  \item Stop if the inputs aren't as expected.
  \ei
}
\item Unit tests
  \bi
  \item For each small function: does it give the right results in
  specific cases?
  \ei
\item Integration tests
  \bi
  \item Check that larger multi-function tasks are working.
  \ei
\item Regression tests
  \bi
  \item Compare output to saved results, to check that things that
    worked continue working.
  \ei
\ei

\note{Your first line of defense should be to include checks of the
  inputs to a function: If they don't meet your specifications, you
  should issue an error or warning.
  But that's not really {\nhilit testing}.

  Your main effort should focus on {\nhilit unit tests}. For each
  small function (and your code {\nhilit should} be organized as a
  series of small functions), write small tests to check that the
  function gives the correct output in specific cases.

  In addition, create larger {\nhilit integration tests} to check that
  larger features are working. It's best to construct these as
  {\nhilit regression tests}: print out the output and check that it
  matches some saved version. This way, if some change you've made
  leads to a change in the results, you'll notice it automatically and
  immediately.
}
\end{frame}




\begin{frame}[c,fragile]{Check inputs}


\begin{lstlisting}
winsorize <-
function(x, q=0.006)
{
  if(!is.numeric(x)) stop("x should be numeric")

  if(!is.numeric(q)) stop("q should be numeric")
  if(length(q) > 1) {
    q <- q[1]
    warning("length(q) > 1; using q[1]")
  }
  if(q < 0 || q > 1) stop("q should be in [0,1]")

  lohi <- quantile(x, c(q, 1-q), na.rm=TRUE)
  if(diff(lohi) < 0) lohi <- rev(lohi)

  x[!is.na(x) & x < lohi[1]] <- lohi[1]
  x[!is.na(x) & x > lohi[2]] <- lohi[2]
  x
}
\end{lstlisting}


\note{
  The {\tt winsorize} function in my R/broman package hadn't included
  any checks that the inputs were okay.

  The simplest thing to do is to include some {\tt if} statements with
  calls to {\tt stop} or {\tt warning}.

  The input {\tt x} is supposed to be a numeric vector, and {\tt q} is
  supposed to be a single number between 0 and 1.
}
\end{frame}




\begin{frame}[c,fragile]{Check inputs}


\begin{lstlisting}
winsorize <-
function(x, q=0.006)
{
  stopifnot(is.numeric(x))
  stopifnot(is.numeric(q), length(q)==1, q>=0, q<=1)

  lohi <- quantile(x, c(q, 1-q), na.rm=TRUE)
  if(diff(lohi) < 0) lohi <- rev(lohi)

  x[!is.na(x) & x < lohi[1]] <- lohi[1]
  x[!is.na(x) & x > lohi[2]] <- lohi[2]
  x
}
\end{lstlisting}


\note{The {\tt stopifnot} function makes this a bit easier.
}
\end{frame}




\begin{frame}[c,fragile]{\href{http://github.com/hadley/assertthat}{\tt assertthat} package}


\begin{lstlisting}
#' import assertthat
winsorize <-
function(x, q=0.006)
{
  if(all(is.na(x)) || is.null(x)) return(x)

  assert_that(is.numeric(x))
  assert_that(is.number(q), q>=0, q<=1)

  lohi <- quantile(x, c(q, 1-q), na.rm=TRUE)
  if(diff(lohi) < 0) lohi <- rev(lohi)

  x[!is.na(x) & x < lohi[1]] <- lohi[1]
  x[!is.na(x) & x > lohi[2]] <- lohi[2]
  x
}
\end{lstlisting}


\note{Hadley Wickham's {\tt assertthat} package adds some functions that
  simplify some of this.

  How is the {\tt assertthat} package used in practice? Look at
  packages which depend on it, such as {\tt dplyr}. Download the
  source for {\tt dplyr} and try {\tt grep assert\_that dplyr/R/*} and
  you'll see a bunch of examples if its use.

  Also try {\tt grep stopifnot dplyr/R/*} and you'll see that both
  are being used.
}
\end{frame}




\begin{frame}{\tt Tests in R packages}

\bbi
\item Examples in {\tt .Rd} files
\item Vignettes
\item {\tt tests/} directory
  \bi
  \item {\tt some\_test.R} and {\tt some\_test.Rout.save}
  \ei
\ei

\vspace{24pt}

\centerline{\hilit {\tt R CMD check} is your friend.}


\note{
  Sure, {\nhilit your} package is correct, but what about all of those
  other R packages? If it's not on CRAN, it's {\nhilit probably crap}.

  The examples and vignettes should focus on helping users to
  understand how to use the code. But since get run whenever
  {\tt R CMD check} is run, they also serve as crude tests that the
  package is working. But not that this is only testing for {\nhilit
  errors}; it's not checking that the code is giving {\nvhilit the
  right answers}.

  Things that you put in the {\tt tests} directory can be more
  rigorous: these are basically regression tests.

  If you want your package on CRAN, you'll need all of these tests and
  examples to be {\nhilit fast}, as CRAN runs {\tt R CMD check} on
  every package every day on multiple systems.
}
\end{frame}


\begin{frame}[c,fragile]{An example example}

\begin{lstlisting}
#' @examples
#' x <- sample(c(1:10, rep(NA, 10), 21:30))
#' winsorize(x, 0.2)
\end{lstlisting}

\note{
  This example doesn't provide a proper {\nhilit test}.  You'll get a
  note if it gives an error, but you don't get any indication about
  whether it's giving the right answer.
}
\end{frame}



\begin{frame}[c,fragile]{A {\tt tests/} example}

\begin{lstlisting}
library(qtl)

# read data
csv <- read.cross("csv", "", "listeria.csv")

# write
write.cross(csv, "csv", filestem="junk")

# read back in
csv2 <- read.cross("csv", "", "junk.csv",
                   genotypes=c("AA", "AB", "BB",
                               "not BB", "not AA"))

# check for a change
comparecrosses(csv, csv2)

unlink("junk.csv")
\end{lstlisting}

\note{
  An advantage of the {\tt tests/} subdirectory is that you can more
  easily test input/output.

  A useful technique here: if you have a pair of functions that are
  the inverse of each other (e.g., write and read), check that if you
  apply one and then the other, you get back to the original.
}
\end{frame}



\begin{frame}{\href{http://github.com/hadley/testthat}{\tt testthat} package}

\vspace{-12pt}

\bbi
\item Expectations
 \bi
 \item[] {\tt expect\_equal(10, 10 + 1e-7)}
 \item[] {\tt expect\_identical(10, 10)}
 \item[] {\tt expect\_equivalent(c("one"=1), 1)}
 \item[] {\tt expect\_warning(log(-1))}
 \item[] {\tt expect\_error(1 + "a")}
 \ei
\item Tests
 \bi
 \item[] {\tt test\_that("winsorize small vectors", \{ ... \})}
 \ei
\item Contexts
 \bi
 \item[] {\tt context("Group of related tests")}
 \ei
\item Store tests in {\tt inst/tests/}
\item {\tt tests/run-all.R} file containing
 \bi
 \item[] {\tt library(testthat) \\
              library({\color{hilit} mypkg}) \\
              test\_package("{\color{hilit} mypkg}")}
 \ei
\ei


\note{
  The {\tt testthat} package greatly simplies unit testing of code in
  R packages.

  There are a bunch of functions for defining ``expectations.''
  Basically, for testing whether something worked as expected. (It can
  be good to check that something gives an error when it's supposed to
  give an error.)

  You then define a set of tests, with a character string to explain
  where the problem is, if there is a problem.

  You can group tests into ``contexts.'' When the tests run, that
  character string will be printed, so you can see what part of the
  code is being tested.

  Put your tests in {\tt .R} files within {\tt inst/tests}. Put
  another file within {\tt tests/} that will ensure that these tests
  are run when you do {\tt R CMD check}.
}
\end{frame}


\begin{frame}[c,fragile]{Example {\tt testthat} test}


\begin{lstlisting}
context("winsorise")

test_that("winsorize works for small vectors", {

  x <-       c(2, 3, 7, 9, 6, NA, 5, 8, NA, 0, 4, 1, 10)
  result1 <- c(2, 3, 7, 9, 6, NA, 5, 8, NA, 1, 4, 1,  7)
  result2 <- c(2, 3, 7, 8, 6, NA, 5, 8, NA, 2, 4, 2,  8)

  expect_identical(winsorize(x, 0.1), result1)
  expect_identical(winsorize(x, 0.2), result2)

})
\end{lstlisting}



\note{
  These are the sort of tests you might do with the testthat
  package. The value of this: finally, we are checking whether the
  code is giving the right answer!

  These are called unit tests. Ideally, you include tests like this
  for every function you write.
}
\end{frame}


\begin{frame}[c,fragile]{Example {\tt testthat} test}


\begin{lstlisting}
test_that("winsorize works for a long vector", {

  set.seed(94745689)
  n <- 1000
  nmis <- 10
  p <- 0.05
  input <- rnorm(n)
  input[sample(1:n, nmis)] <- NA
  quL <- quantile(input, p, na.rm=TRUE)
  quH <- quantile(input, 1-p, na.rm=TRUE)

  result <- winsorize(input, p)
  middle <- !is.na(input) & input >= quL & input <= quH
  low <- !is.na(input) & input <= quL
  high <- !is.na(input) & input >= quH

  expect_identical(is.na(input), is.na(result))
  expect_identical(input[middle], result[middle])
  expect_true( all(result[low] == quL) )
  expect_true( all(result[high] == quH) )

})
\end{lstlisting}



\note{
  Here's a bigger, more interesting test.

  The code to test a function will generally be longer than the
  function itself.
}
\end{frame}


\begin{frame}[c,fragile]{Workflow}

\bbi
\item Write tests as you're coding.
\item {\tt test()}
 \bi
 \item with {\tt devtools}, and working in your package directory
 \ei
\item {\tt auto\_test("R", "inst/tests")}
 \bi
 \item automatically runs tests when any file changes
 \ei
\ei

\note{
  Read Hadley's paper about {\tt testthat}. It's pretty easy to
  incorporate testing into your development workflow.
}
\end{frame}


\begin{frame}[c]{Debugging}

\centerline{Step 1: Reproduce the problem}

\vspace{24pt}

\onslide<2->{\centerline{Step 2: Turn it into a test}}


\note{Try to create the minimal example the produces the problem. This
  helps both for refining your understanding of the problem and for
  speed in testing.

  Once you've created a minimal example that produces the problem,
  {\nhilit add that to your battery of automated tests!} The problem
  may suggest related tests to also add.
}
\end{frame}



\begin{frame}[c,fragile]{What to test?}

\bbi
\item You can't test {\hilit everything}.
\item Focus on the {\hilit boundaries}
   \bi
   \item Vector of length 0 or 1
   \item ...
   \ei
\item Test handling of missing data.
\ei

\note{
}
\end{frame}



\begin{frame}{Learn to use debugging tools}


\bbi
\item {\tt cat}
\item {\tt traceback}
\item RStudio
\item Eclipse
\item gdb
\ei

\note{
}
\end{frame}



\begin{frame}[c]{Debugging}


\centerline{Isolate the problem: where do things go bad?}

\note{``Divide and conquer.''
}
\end{frame}




\begin{frame}[c]{Debugging}


\centerline{Don't make the same mistake twice.}

\note{If you figure out some mistake you've made, search for all other
  possible instances of that mistake.
}
\end{frame}









\begin{frame}[c]{The most pernicious bugs}

\centerline{The code is right, but your thinking is wrong}

\vspace{24pt}

\onslide<2->{\centerline{You don't understand your programming language}}

\vspace{24pt}

\onslide<3->{\centerline{\hilit $\rightarrow$ Write trivial programs to
    test your understanding.}}

\note{Consider that your algorithm may be garbage.
Try an independent implementation.
}
\end{frame}




\begin{frame}[c]{Profiling}

\centerline{Don't try to optimize until the {\hilit very} end}


\note{
}
\end{frame}




\end{document}
